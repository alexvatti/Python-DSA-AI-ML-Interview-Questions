# -*- coding: utf-8 -*-
"""Single-Head-Multi-Head-Transformer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V08Kfg1ODcAT2A9TLp7qhGRDU0BzDTrz

# Transformer (Block) with Sinlge Head Attention
"""

pip install gensim

"""## Import the Necessay Packages"""

import numpy as np
import gensim.downloader as api
import matplotlib.pyplot as plt

class Single_Head_Transfomer():
  def __init__(self):
    self.num_heads=1
    self.model = api.load("glove-wiki-gigaword-50")
    self.dim = 50

  def layer_norm(self, X, eps=1e-6):
    """Applies Layer Normalization"""
    mean = X.mean(axis=-1, keepdims=True)
    std = X.std(axis=-1, keepdims=True)
    return (X - mean) / (std + eps)

  def feed_forward(self, X):
    """Applies Feed Forward Network: Linear -> ReLU -> Linear"""
    np.random.seed(42)
    hidden_dim = self.dim * 2  # typically 2–4× the embedding size
    W1 = np.random.randn(self.dim, hidden_dim)
    b1 = np.random.randn(hidden_dim)
    W2 = np.random.randn(hidden_dim, self.dim)
    b2 = np.random.randn(self.dim)

    return np.maximum(0, X @ W1 + b1) @ W2 + b2  # ReLU + second linear

  def single_head_attention(self, sentence):
    words = sentence.split()
    X = np.array([self.model[word] for word in words])
    np.random.seed(42)
    head_dim = X.shape[1] // self.num_heads

    # Random weights for Q, K, V
    W_Q = np.random.rand(X.shape[1], head_dim)
    W_K = np.random.rand(X.shape[1], head_dim)
    W_V = np.random.rand(X.shape[1], head_dim)

    Q = X @ W_Q
    K = X @ W_K
    V = X @ W_V

    # Attention scores
    scores = Q @ K.T / np.sqrt(head_dim)
    exp_x = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
    attn_weights = exp_x / np.sum(exp_x, axis=-1, keepdims=True)

    # Weighted sum
    attn_output = attn_weights @ V

    # ---- Add & Norm ----
    X_norm = self.layer_norm(X + attn_output)

    # ---- Feed Forward + Add & Norm ----
    ffn_output = self.feed_forward(X_norm)
    output = self.layer_norm(X_norm + ffn_output)

    print("Original shape:", X.shape)
    print("After Attention:", attn_output.shape)
    print("After FFN:", output.shape)
    print("\nFirst word original (first 5 dims):", X[0][:5])
    print("First word after Transformer block (first 5 dims):", output[0][:5])

    plt.figure(figsize=(5, 4))
    plt.imshow(attn_weights, cmap='coolwarm')
    plt.xticks(range(len(words)), words, rotation=45)
    plt.yticks(range(len(words)), words)
    plt.colorbar(label="Attention Weight")
    plt.title("Single-Head Attention Heatmap")
    plt.show()




single_head=Single_Head_Transfomer()
single_head.single_head_attention("the cat sat on table")

"""# Multi Head Attention (Transformer)"""

import numpy as np
import gensim.downloader as api
import matplotlib.pyplot as plt

class Multi_Head_Transformer():
  def __init__(self, num_heads=5):
    self.num_heads = num_heads
    self.model = api.load("glove-wiki-gigaword-50")
    self.dim = 50
    assert self.dim % self.num_heads == 0, "dim must be divisible by num_heads"
    self.head_dim = self.dim // self.num_heads

  def layer_norm(self, X, eps=1e-6):
    mean = X.mean(axis=-1, keepdims=True)
    std = X.std(axis=-1, keepdims=True)
    return (X - mean) / (std + eps)

  def feed_forward(self, X):
    np.random.seed(42)
    hidden_dim = self.dim * 2
    W1 = np.random.randn(self.dim, hidden_dim)
    b1 = np.random.randn(hidden_dim)
    W2 = np.random.randn(hidden_dim, self.dim)
    b2 = np.random.randn(self.dim)
    return np.maximum(0, X @ W1 + b1) @ W2 + b2

  def multi_head_attention(self, sentence):
    words = sentence.split()
    X = np.array([self.model[word] for word in words])
    np.random.seed(42)

    # Weight matrices for all heads
    W_Q = np.random.randn(self.num_heads, self.dim, self.head_dim)
    W_K = np.random.randn(self.num_heads, self.dim, self.head_dim)
    W_V = np.random.randn(self.num_heads, self.dim, self.head_dim)

    heads_output = []
    attn_all = []

    for i in range(self.num_heads):
      Q = X @ W_Q[i]
      K = X @ W_K[i]
      V = X @ W_V[i]

      scores = Q @ K.T / np.sqrt(self.head_dim)
      exp_x = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
      attn_weights = exp_x / np.sum(exp_x, axis=-1, keepdims=True)

      attn_all.append(attn_weights)
      head_output = attn_weights @ V
      heads_output.append(head_output)

    # Concatenate all heads and project back
    concat_output = np.concatenate(heads_output, axis=-1)
    W_O = np.random.randn(self.dim, self.dim)
    multi_head_output = concat_output @ W_O

    # Add & Norm
    X_norm = self.layer_norm(X + multi_head_output)

    # Feed Forward + Add & Norm
    ffn_output = self.feed_forward(X_norm)
    output = self.layer_norm(X_norm + ffn_output)

    # --- Visualization ---
    print("Original shape:", X.shape)
    print("After Multi-Head Attention:", multi_head_output.shape)
    print("After FFN:", output.shape)
    print("\nFirst word original (first 5 dims):", X[0][:5])
    print("First word after Transformer block (first 5 dims):", output[0][:5])

    fig, axes = plt.subplots(1, self.num_heads, figsize=(16, 4))
    for i, attn in enumerate(attn_all):
      axes[i].imshow(attn, cmap='coolwarm')
      axes[i].set_title(f'Head {i+1}')
      axes[i].set_xticks(range(len(words)))
      axes[i].set_xticklabels(words, rotation=45)
      axes[i].set_yticks(range(len(words)))
      axes[i].set_yticklabels(words)
    plt.tight_layout()
    plt.show()


# Example Run
multi_head = Multi_Head_Transformer(num_heads=5)
multi_head.multi_head_attention("the cat sat on the table")









